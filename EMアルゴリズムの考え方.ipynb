{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMアルゴリズム"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数式テンプレート\n",
    "```math\n",
    "\\begin{matrix}\n",
    "テンプレート\n",
    "\\end{matrix}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カルバック・ライブラーダイバージェンス\n",
    "+ カルバック・ライブラー情報量\n",
    "+ 相互情報量\n",
    "+ ゼロ以上のスカラー値\n",
    "+ 同じ定義域のxに対して確率分布$p(x)$と$q(x)$が等しい時ゼロになる\n",
    "+ 確率分布pとqの距離\n",
    "\n",
    "```math\n",
    "\\begin{matrix}\n",
    "D_{KL}(p||q) &\\neq& D_{KL}(q||p) \\\\\n",
    "D_{KL}(p||q) &=& \\int p(x)log\\frac{p(x)}{q(x)} \\geq 0\n",
    "\\end{matrix}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カルバック・ライブラー情報量の最小化は, 最尤推定に等しい\n",
    "#### モンテカルロ推定によってカルバック・ライブラー情報量を求める\n",
    "```math\n",
    "\\begin{matrix}\n",
    "D_{KL}(p||q) &=& \\int p(x)log\\frac{p(x)}{q(x)}\n",
    "&=& E_{p}[log\\frac{p(x)}{q(x)}]\n",
    "\\end{matrix}\n",
    "```\n",
    "\n",
    "```math\n",
    "\\begin{matrix}\n",
    "D_{KL}(p||q) &\\sim& \\frac{1}{N}log\\frac{p(x^{n})}{q_{\\theta}(x^{n})} \\\\\n",
    "&=& \\frac{1}{N}\\sum_{n=1}^{N}(logP(x^{n}) - logP_{\\theta}(x^{x}))\n",
    "\\end{matrix}\n",
    "```\n",
    "+ $\\theta$を含まない$logP(x^{n})$を除外\n",
    "\n",
    "```math\n",
    "\\begin{matrix}\n",
    "\\underset{\\theta} {\\operatorname{argmin}} D_{KL}(p||q) \\sim \\underset{\\theta} {\\operatorname{argmin}} (-\\frac{1}{N}\\sum_{n=1}^{N}logP_{\\theta}(x^{n}))\n",
    "= \\underset{\\theta} {\\operatorname{argmax}} \\sum_{n=1}^{N}logP_{\\theta}(x^{n})\n",
    "\\end{matrix}\n",
    "```\n",
    "+ $\\underset{\\theta} {\\operatorname{argmax}}\\sum_{n=1}^{N}logP_{\\theta}(x^{n})$ は最尤推定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 潜在変数を持つモデル\n",
    "\n",
    "+ 潜在変数$z_{k}^{n} \\sim q(z_{k}^{n} | x^{n})$ を持つ尤度\n",
    "+ $q(z_{k}^{n} | x^{n})$は, 任意の確率変数\n",
    "\n",
    "### 潜在変数を持つモデルの尤度\n",
    "+ 同時確率$P(x,z)$のzに関する周辺化\n",
    "```math\n",
    "logP_{\\theta}(x) = log\\sum_{k=1}^{K}P_{\\theta}(x, z)\n",
    "```\n",
    "\n",
    "### 潜在変数を持つ対数尤度\n",
    "```math\n",
    "\\begin{matrix}\n",
    "D &=& \\set{ x^{1}, x^{2}, \\cdots x^{n} } \\\\\n",
    "logP_{\\theta}(D) &=& \\sum_{n=1}^{N}logP_{\\theta}(x^{n}) \\\\\n",
    "&=& \\sum_{n=1}^{N}log\\sum_{k=1}^{K}P_{\\theta}(x^{n}, z^{n})\n",
    "\\end{matrix}\n",
    "```\n",
    "+ 潜在変数を持つ対数尤度は`log-sum`の形式をしている\n",
    "+ `log-sum`形式は非常に厄介.\n",
    "+ EMアルゴリズムは, `log-sum`形式を`sum-log`形式に変換して解決する\n",
    "\n",
    "### $P_{\\theta}(x^{n})$の$P_{\\theta}(z_{k}^{n} | x^{n})$を任意の確率分布$q(z_{k}^{n})$で近似する\n",
    "+ $P_{\\theta}(x^{n})$が厄介者\n",
    "+ $q(z)$を導入して潜在変数を持つ尤度モデルを解ける形式に変換する\n",
    "\n",
    "```math\n",
    "\\begin{matrix}\n",
    "P_{\\theta}(z_k^{n}|x^{n}) &=& \\frac{P_{\\theta}(x^{n}, z_{k}^{n})}{\\sum_{k=1}^{K}P_{\\theta}(x^{\\theta}, z_{\\theta}^{n})} \\\\\n",
    "&=& log\\frac{P_{\\theta}(x_{\\theta}^{n}, z_{\\theta})}{P_{\\theta}(z_{k}^{n}|x^{n})} \\cdot \\frac{q(z_{k}^{n})}{q(z_{k}^{n})} \\\\\n",
    "&=& log\\frac{P_{\\theta}(x^{n}, z_{k}^{n})}{q(z_{k}^{n})} + log\\frac{q(z_{k}^{n})}{P_{\\theta}(z_{k}^{n} | x^{x})}\n",
    "\\end{matrix}\n",
    "```\n",
    "\n",
    "```math\n",
    "\\begin{matrix}\n",
    "logP_{\\theta}(x^{n}) &=& logP_{\\theta}(x^{n})\\sum_{k=1}^{K}q(z_{k}^{n}) \\\\\n",
    "&=& \\sum_{k=1}^{K}q(z_{k}^{n}) \\cdot logP_{\\theta}(x^{n}) \\\\\n",
    "&=& \\sum_{k=1}^{K}q(z_{k}^{n})(log\\frac{P_{\\theta}(x^{n}, z_{k}^{n})}{q(z_{k}^{n})} + log\\frac{q(z_{k}^{n})}{P_{\\theta}(x^{n} | z_{k}^{n})})\n",
    "\\end{matrix}\n",
    "```\n",
    "\n",
    "### 潜在変数を持つ尤度は, ELBOとKL情報量の和に帰着する\n",
    "```math\n",
    "\\begin{matrix}\n",
    "logP_{\\theta}(x^{n}) &=& \\sum_{k=1}^{K}q(z_{k}^{n}) \\cdot log\\frac{P_{\\theta}(x^{n}, z_{k}^{n})}{q(z_k)} + \\sum_{k=1}^{K}D_{KL}(q(z_{k}^{n})||P_{\\theta}(z_{k}^{n})) \\\\\n",
    "ELBO(x^{n}; q(z^{n}), \\theta) &=& \\sum_{k=1}^{K}q(z_{k}^{n}) \\cdot log\\frac{P_{\\theta}(x^{n}, z_{k}^{n})}{q(z_{k}^{n})} \\\\\n",
    "D_{KL}(q(z^{n})||P_{\\theta}(z^{n}|x^{n})) &=& \\sum_{k=1}^{K}D_{KL}(q(z_{k}^{n})||P_{\\theta}(z_{k}^{n}|x^{n}))\n",
    "\n",
    "\\end{matrix}\n",
    "```\n",
    "\n",
    "```math\n",
    "\\begin{matrix}\n",
    "logP_{\\theta}(x^{n}) &=& ELBO(x^{n}; q(z^{n}), \\theta) + D_{KL}(q(z^{n})||P_{\\theta}(z^{n}|x^{n})) \\\\\n",
    "\n",
    "logP_{\\theta}(D) &=& \\sum_{n=1}^{N}ELBO(x^{n}; q(z^{n}), \\theta) + \\sum_{n=1}^{N}D_{KL}(q(z^{n})||P_{\\theta}(z^{n}|x^{n}))\n",
    "\\end{matrix}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMアルゴリズム手順\n",
    "1. Eステップ: $\\set{q^{1}, q^{2}, q^{3}, \\cdots, q^{K}}$ の更新. $\\theta$ は固定.\n",
    "+ 各nに対して $q^{n}(z)=P_{\\theta}(z|x^{n})$ とする\n",
    "2. Mステップ: 初期値 $\\theta=\\theta_{old}$ から最大値となる $\\theta_{update}$ に更新.\n",
    "+ $\\set{q^{1}, q^{2}, q^{3}, \\cdots, q^{K}}$ は固定.\n",
    "+ $\\sum_{n=1}^{N}ELBO(x^{n}; q^{n}, \\theta)$ が最大になる $\\theta$ を解析的に求める.\n",
    "3. 終了判定: 対数尤度の平均 $\\frac{1}{N}\\sum_{n=1}^{N}logP(x^{n}; \\theta)$ を計算して前回の平均対数尤度と比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
